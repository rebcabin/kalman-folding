#+TITLE: Kalman Folding 3: Derivations (Review Draft)
#+SUBTITLE: Extracting Models from Data, One Observation at a Time
#+AUTHOR: Brian Beckman
#+DATE: <2016-05-03 Tue>
#+EMAIL: bbeckman@34363bc84acc.ant.amazon.com
#+OPTIONS: ':t *:t -:t ::t <:t H:3 \n:nil ^:t arch:headline author:t c:nil
#+OPTIONS: d:(not "LOGBOOK") date:t e:t email:nil f:t inline:t
#+OPTIONS: num:t p:nil pri:nil stat:t tags:t tasks:t tex:t timestamp:t toc:t
#+OPTIONS: todo:t |:t
#+SELECT_TAGS: export
#+STARTUP: indent
#+LaTeX_CLASS_OPTIONS: [10pt,oneside,x11names]
#+LaTeX_HEADER: \usepackage{geometry}
#+LaTeX_HEADER: \usepackage{amsmath}
#+LaTeX_HEADER: \usepackage{amssymb}
#+LaTeX_HEADER: \usepackage{amsfonts}
#+LaTeX_HEADER: \usepackage{palatino}
#+LaTeX_HEADER: \usepackage{siunitx}
#+LaTeX_HEADER: \usepackage{esdiff}
#+LaTeX_HEADER: \usepackage{xfrac}
#+LaTeX_HEADER: \usepackage{nicefrac}
#+LaTeX_HEADER: \usepackage{faktor}
#+LaTeX_HEADER: \usepackage[euler-digits,euler-hat-accent]{eulervm}
#+OPTIONS: toc:3

* COMMENT Preliminaries

This section is just about setting up org-mode. It shouldn't export to the
typeset PDF and HTML.

#+BEGIN_SRC emacs-lisp :exports results none
  (defun update-equation-tag ()
    (interactive)
    (save-excursion
      (goto-char (point-min))
      (let ((count 1))
        (while (re-search-forward "\\tag{\\([0-9]+\\)}" nil t)
          (replace-match (format "%d" count) nil nil nil 1)
          (setq count (1+ count))))))
  (update-equation-tag)
  (setq org-confirm-babel-evaluate nil)
  (org-babel-map-src-blocks nil (org-babel-remove-result))
  (slime)
#+END_SRC

#+RESULTS:
: #<buffer *inferior-lisp*>

* Abstract

In /Kalman Folding, Part 1/,[fn:klf1] we present basic, static Kalman filtering
as a functional fold, highlighting the unique advantages of this form for
deploying test-hardened code verbatim in harsh, mission-critical environments.
The examples in that paper are all static, meaning that the states of the model
do not depend on the independent variable, often physical time.

Here, we present mathematical derivations of the basic, static filter. These are
semi-formal sketches that leave many details to the reader, but highlight all
important points that must be rigorously proved. These derivations have several
novel arguments and we strive for much higher clarity and simplicity than is
found in most treatments of the topic.

* Kalman Folding 

#+BEGIN_COMMENT
In this series of papers, we use the Wolfram language[fn:wolf] because it excels
at concise expression of mathematical code. All examples in these papers can be
directly transcribed to any modern mainstream language that supports closures.
For example, it is easy to write them in C++11 and beyond, Python, any modern
Lisp, not to mention Haskell, Scala, Erlang, and OCaml. Many can be written
without full closures; function pointers will suffice, so they are easy to write
in C. It's also not difficult to add extra arguments to simulate just enough
closure-like support in C to write the rest of the examples in that language.
#+END_COMMENT

In /Kalman Folding, Part 1/,[fn:klf1] we found the following small formulation for the
accumulator function of a fold that implements the static Kalman filter:

#+BEGIN_LaTeX
\begin{equation}
\label{eqn:kalman-cume-definition}
\text{kalmanStatic}
\left(
\mathbold{Z}
\right)
\left(
\left\{
\mathbold{x},
\mathbold{P}
\right\},
\left\{
\mathbold{A},
\mathbold{z}
\right\}
\right) =
\left\{
\mathbold{x}+
\mathbold{K}\,
\left(
\mathbold{z}-
\mathbold{A}\,
\mathbold{x}
\right),
\mathbold{P}-
\mathbold{K}\,
\mathbold{D}\,
\mathbold{K}^\intercal
\right\}
\end{equation}
#+END_LaTeX

\noindent where

#+BEGIN_LaTeX
\begin{align}
\label{eqn:kalman-gain-definition}
\mathbold{K}
&=
\mathbold{P}\,
\mathbold{A}^\intercal\,
\mathbold{D}^{-1} \\
\label{eqn:kalman-denominator-definition}
\mathbold{D}
&= \mathbold{Z} +
\mathbold{A}\,
\mathbold{P}\,
\mathbold{A}^\intercal
\end{align}
#+END_LaTeX

\noindent and all quantities are matrices:

- $\mathbold{z}$ is a  ${b}\times{1}$ column vector containing one multidimensional observation
- $\mathbold{x}$ is an ${n}\times{1}$ column vector of /model states/
- $\mathbold{Z}$ is a  ${b}\times{b}$ matrix, the covariance of
  observation noise
- $\mathbold{P}$ is an ${n}\times{n}$ matrix, the theoretical
  covariance of $\mathbold{x}$
- $\mathbold{A}$ is a  ${b}\times{n}$ matrix, the /observation partials/
- $\mathbold{D}$ is a  ${b}\times{b}$ matrix, the Kalman denominator
- $\mathbold{K}$ is an ${n}\times{b}$ matrix, the Kalman gain

In physical or engineering applications, these quantities carry physical
dimensions of units of measure in addition to their matrix dimensions as numbers
of rows and columns. 
If the physical and matrix dimensions of 
$\mathbold{x}$ 
are
$\left[\left[\mathbold{x}\right]\right]
\stackrel{\text{\tiny def}}{=}
(\mathcal{X}, n\times{1})$
and of 
$\mathbold{z}$ 
are
$\left[\left[\mathbold{z}\right]\right]
\stackrel{\text{\tiny def}}{=}
(\mathcal{Z}, b\times{1})$, then

#+BEGIN_LaTeX
\begin{equation}
\label{eqn:dimensional-breakdown}
\begin{array}{lccccr}
\left[\left[\mathbold{Z}\right]\right]                                       &=& (&\mathcal{Z}^2            & b\times{b}&) \\
\left[\left[\mathbold{A}\right]\right]                                       &=& (&\mathcal{Z}/\mathcal{X}  & b\times{n}&) \\
\left[\left[\mathbold{P}\right]\right]                                       &=& (&\mathcal{X}^2            & n\times{n}&) \\
\left[\left[\mathbold{A}\,\mathbold{P}\,\mathbold{A}^\intercal\right]\right] &=& (&\mathcal{Z}^2            & b\times{b}&) \\
\left[\left[\mathbold{D}\right]\right]                                       &=& (&\mathcal{Z}^2            & b\times{b}&) \\
\left[\left[\mathbold{P}\,\mathbold{A}^\intercal\right]\right]               &=& (&\mathcal{X}\,\mathcal{Z} & n\times{b}&) \\
\left[\left[\mathbold{K}\right]\right]                                       &=& (&\mathcal{X}/\mathcal{Z}  & n\times{b}&)
\end{array}
\end{equation}
#+END_LaTeX

Dimensional arguments, regarding both matrix dimensions and physical dimensions,
are invaluable for checking the derivations that follow.

* Derivations

Below, we derive equations \ref{eqn:kalman-cume-definition},
\ref{eqn:kalman-gain-definition} and \ref{eqn:kalman-denominator-definition}.
Again, these derivations are just sketches designed for clarity as opposed to
rigorous proofs.
These derivations only cover the
static Kalman filter, where $\mathbold{x}$ are
fixed, constant, static states of the model. See Bar-Shalom[fn:bars] for
derivations of the Kalman filter with time-dependent states and part 2 of this series[fn:klf2] for
an example.

The plan is first to develop expressions for the prior estimate
$\tilde{\mathbold{x}}$ and prior covariance $\tilde{\mathbold{P}}$, and then expressions
for the posterior versions $\hat{\mathbold{x}}$ and $\hat{\mathbold{P}}$,
defining the Kalman gain $\mathbold{K}$ matrix and the denominator matrix
$\mathbold{D}$ along the way. Finally, we derive the particular, convenient expressions for $\mathbold{K}$
and $\mathbold{D}$ that appear in equations \ref{eqn:kalman-cume-definition},
\ref{eqn:kalman-gain-definition}, and \ref{eqn:kalman-denominator-definition}.
Bierman laid out this strategy for the derivation in his classic book
/Factorization Methods for Discrete Sequential Estimation/.[fn:bier] We follow
his plan, unpacking many of his elided steps for greater clarity.

** Notation

The word /vector/ alone means /column vector/ by default. If a quantity is a row
vector, we explicitly say so. In general, lower-case boldface symbols like
$\mathbold{x}$ denote column vectors. Row vectors include a superscript
/transpose/ symbol, as in $\mathbold{a}^\intercal$. We write literal vectors in
square brackets, as in $\left[a, b, \ldots\right]^\intercal$ for a column vector
or $\left[a, b, \ldots\right]$ for a row vector or for cases where we don't care
whether it's a column or row.


Upper-case
boldface symbols like $\mathbold{M}$ denote matrices. Our matrices are not
always square. Because vectors are
special cases of matrices, some matrices are also vectors. We may use an
upper-case symbol to denote a vector, but we do not use a lower-case symbol to
denote a non-vector matrix.

Juxtaposition, as in
$\mathbold{A}\,\mathbold{x}$ or $\mathbold{A}\,\mathbold{B}$, means matrix
multiplication. 
When we write a product like
$\mathbold{A}\,\mathbold{B}$, we assume that the number of columns of
$\mathbold{A}$ matches the number of rows of $\mathbold{B}$. 

Matrix multiplication is non-commutative, meaning that
$\mathbold{A}\,\mathbold{B}$ does not, in general, equal
$\mathbold{B}\,\mathbold{A}$. However, if a matrix $\mathbold{D}$ is square and
diagonal,
meaning that it has non-zero entries only along its main diagonal from upper
left to lower right, then $\mathbold{A}\,\mathbold{D}$ does always equal
$\mathbold{D}\,\mathbold{A}$. We may freely use this fact without mentioning it
explicitly.

Symmetric matrices do not always
commute, even with other symmetric matrices. In particular, the product of two
symmetric matrices is not always symmetric, as witnessed by the following
counterexample:

#+BEGIN_LaTeX
\begin{equation*}
\left(
\begin{array}{cc}
 1 & 2 \\
 2 & 3 \\
\end{array}
\right)\cdot\left(
\begin{array}{cc}
 4 & 5 \\
 5 & 6 \\
\end{array}
\right)
=
\left(
\begin{array}{cc}
 14 & 17 \\
 23 & 28 \\
\end{array}
\right)
\end{equation*}
#+END_LaTeX


Matrix multiplication is associative, meaning that the order in which pairwise
multiplications is carried out does not matter. Thus 

\[(\mathbold{A}\,\mathbold{B})\,\mathbold{C}=\mathbold{A}\,(\mathbold{B}\,\mathbold{C})=\mathbold{A}\,\mathbold{B}\,\mathbold{C}\]

\noindent and we don't need to write parentheses. That means
some expressions of long products can be hard to read. We occasionally use
a center dot or $\times$ symbol to make multiplication easier to read, as in
$\mathbold{A}\cdot\mathbold{x}$ or $\mathbold{A}\times\mathbold{x}$. We also use
the $\times$ symbol when discussing the numbers of rows and columns of a matrix,
as in ``$\mathbold{A}$ is an $m\times n$ matrix,'' meaning that $\mathbold{A}$
has $m$ rows and $n$ columns.


We may freely  exploit the following facts without mentioning them explicitly:
- For any matrix $\mathbold{M}$, $\left(\mathbold{M}^\intercal\right)^\intercal = \mathbold{M}$
- For any invertible matrix $\mathbold{M}$, $\left(\mathbold{M}^{-1}\right)^{-1} = \mathbold{M}$
- For any two matrices $\mathbold{A}$ and
  $\mathbold{B}$,
  $\left(\mathbold{A}\,\mathbold{B}\right)^\intercal=\mathbold{B}^\intercal\mathbold{A}^\intercal$
- $\left(\mathbold{A}\,\mathbold{B}\right)^{-1}=\mathbold{B}^{-1}\mathbold{A}^{-1}$
  when the matrices are invertible
- $\mathbold{P}^\intercal$ = $\mathbold{P}$ if and only if $\mathbold{P}$ is
  symmetric

For any matrix $\mathbold{M}$, $\mathbold{M}^2$ means
$\mathbold{M}^\intercal\mathbold{M}$, the transpose of the matrix times the
matrix. Such squared matrices are always square and symmetric.
This notation pertains to vectors, as well, because they are just
special cases of matrices. Thus,
$\mathbold{x}^2=\mathbold{x}^\intercal\mathbold{x}$, the square of the Euclidean
$\mbox{2-\textrm{norm}}$ of $\mathbold{x}$, a scalar; and
$(\mathbold{x}^\intercal)^2 =
(\mathbold{x}^\intercal)^\intercal\cdot
\mathbold{x}^\intercal=
\mathbold{x}\,\mathbold{x}^\intercal$
is the outer product of $\mathbold{x}$ with itself; that outer product is an
$n\times{n}$ square, symmetric matrix, where $n$ is the dimensionality of $\mathbold{x}$. 



When $\mathbold{M}^2$ is invertible, $\mathbold{M}^{-2}$
means the inverse of $\mathbold{M}^2$, namely
$\left(\mathbold{M}^\intercal\mathbold{M}\right)^{-1}$.

We use the term /tall/ to mean a matrix with more rows than columns, that is, an
$m\times{n}$
matrix when
$m>n$. When discussing
$m\times{n}$
matrices, we  usually assume that
$m>n$.
We use the term /wide/ to mean a matrix with
more columns than rows, as in an $n\times{m}$ matrix. We use the term /small/ to
mean $n\times{n}$, and /large/ to mean $m\times{m}$. 

*** Probability and Statistics

We use the terms /distribution/ and /expectation value/ without definition in
this paper. If $\mathbold{x}$ is a random variable, then we denote the
expectation value of some function $f$ of $\mathbold{x}$ as $E[f(\mathbold{x})]$.

** Definitions

- $t$ :: is the independent variable. In many applications, $t$ represents physical
     time, or an integer index mapped to physical time. It is known and
     non-random. We treat it as a scalar, here, though it is possible to extend
     the theory to a vector $t$.

- $\mathbold{x}$ :: is the (column) vector of $n$ unknown, constant /states/
     of the model. It's a random variable, and we compute estimates and
     covariances /via/ expectation values over its distribution. This symbol
     also means an algebraic variable standing for some particular estimate of
     the states.

- $\mathbold{A}\,\mathbold{x}$ :: is the /model/; it predicts an observation at
     time $t$ given an estimate of the states $\mathbold{x}$ and a current
     partials matrix $\mathbold{A}$ that may depend on $t$. The model is a
     column vector of dimensionality $b\times{1}$, the same as the dimensionality of an
     observation $\mathbold{z}$.

- $\mathbold{A}$ :: is the /current partials matrix/, the partial derivative of
     the model with respect to the unknown states $\mathbold{x}$, evaluated
     at the current value of the independent variable $t$. We could write
     $\mathbold{A}$ as $\mathbold{A}(t)$; it's an
     aesthetic judgment to omit explicit $t$ dependence because it
     would make the derivations longer and harder to read.  Because the
     model is /linear/, the partials do not depend on $\mathbold{x}$. 
     $\mathbold{A}$ is known, non-random, and may depend on $t$. Generally,
     its dimensionality
     is $b\times{n}$, where $b$ is the dimensionality of an 
     observation $\mathbold{z}$.

- $\tilde{\mathbold{A}}$ :: is the /prior partials matrix/, a matrix that stacks
     all the prior rows of $\mathbold{A}$ that precede the current row. It is
     known, non-random, and $m b\times{n}$, where $m$ is the number of prior
     observations, $b$ is the dimensionality of a single
     observation $\mathbold{z}$, and $n$ is the dimensionality of the states
     $\mathbold{x}$.  Thus
     $\tilde{\mathbold{A}}$ is tall in the typical /overdetermined/ case where
     $m>n$, more observations than states. We do not actually
     realize $\tilde{\mathbold{A}}$ in computer memory because Kalman keeps
     /all information/ in the running covariance matrix. $\tilde{\mathbold{A}}$
     is just a
     useful abstraction for the derivations below.

- $\mathbold{z}$ :: is the /current observation/. It is known and non-random.
     Its dimensionality is $b\times{1}$.

- $\tilde{\mathbold{z}}$ :: is a stack of all prior observations. It
     is known, non-random, $m b\times{1}$. It's a useful abstraction in the
     derivations below. It's not necessary to actually realize it in computer
     memory because we use all its information incrementally by folding.

- ${\tilde{\mathbold{x}}}$ :: the /prior estimate/, the estimate of
     $\mathbold{x}$ given all information we have prior to the current
     observation. It is known, non-random, $n\times{1}$. 

- ${\hat{\mathbold{x}}}$ ::  the /posterior estimate/, the estimate of
     $\mathbold{x}$ given (1) the prior estimate ${\tilde{\mathbold{x}}}$, (2)
     the current partials $\mathbold{A}$, and (3) the current observation
     $\mathbold{z}$. It is known, non-random, $n\times{1}$. It satisfies
     /the Kalman update equation/:

#+BEGIN_LaTeX
\begin{equation}
\label{eqn:kalman-update-equation}
{\hat{\mathbold{x}}} =
{\tilde{\mathbold{x}}} +
\mathbold{K}
\left(
\mathbold{z}-
\mathbold{A}\,
{\tilde{\mathbold{x}}}
\right)
\end{equation}
#+END_LaTeX

\noindent which is equivalent to  the recurrence
$\mathbold{x}\leftarrow\mathbold{x}+\mathbold{K}\,(z-\mathbold{A}\,\mathbold{x})$
used in part 1 of this series.

- ${\tilde{\mathbold{P}}}$ :: /covariance of the priors/, equals
     ${\tilde{\mathbold{A}}}^{-2}$
     (de-dimensionalized; proof sketch
     below). This is called just $\mathbold{P}$ in part one of this series.
     It is known, non-random, $n\times{n}$. 

- ${\hat{\mathbold{P}}}$ :: /posterior covariance/, satisfies
     ${\hat{\mathbold{P}}}\,
     {\mathbold{A}}^\intercal=
     \mathbold{K}=
     {\tilde{\mathbold{P}}}\,\mathbold{A}^\intercal\,\mathbold{D}^{-1}$
     (de-dimensionalized; proof sketch below). We calculate it from the prior covariance
     $\tilde{\mathbold{P}}$ and the new
     partials matrix $\mathbold{A}$. 
     It is known, non-random, $n\times{n}$. 

- $\mathbold{A}\,{\tilde{\mathbold{x}}}$ :: the /predicted observation/ given
     the prior estimate ${\tilde{\mathbold{x}}}$ and the current partials matrix
     $\mathbold{A}$. It is a particular evaluation of the model. It is known,
     non-random, $b\times{1}$.

- $\mathbold{z}-\mathbold{A}\,{\tilde{\mathbold{x}}}$ ::  the measurement
     /residual/, the difference between the current observation $\mathbold{z}$ and the
     predicted observation $\mathbold{A}\,{\tilde{\mathbold{x}}}$.

- $\mathbold{\zeta}$ ::  /observation noise/: random column-vector with
     zero mean and covariance $\mathbold{Z}$ (unity, $\mathbold{1}$, after
     de-dimensionalization).
     It has $b$ rows and $1$ column, like $\mathbold{z}$. 

- $\mathbold{Z}$ :: covariance of the observation noise, $E
     \left[
     \mathbold{\zeta}\,
     \mathbold{\zeta}^\intercal
     \right]$: known, non-random $b\times{b}$.

- $\tilde{\mathbold{z}} = \tilde{\mathbold{A}}\,{\mathbold{x}} + \mathbold{\zeta}$ :: the
     /observation equation/, which equates $\tilde{\mathbold{z}}$, the stack of
     all prior observations, to the product of $\tilde{\mathbold{A}}$, the stack
     of all prior partials matrices, and an unknown random vector of states,
     $\mathbold{x}$, plus some unknown random observation noise
     $\mathbold{\zeta}$.  The stack of prior observations
     $\tilde{\mathbold{z}}$ is known, non-random, $m b\times{1}$; the stack of prior
     partials matrices
     $\tilde{\mathbold{A}}$ is known, non-random, $m b\times{n}$; the state vector ${\mathbold{x}}$
     is unknown, random, $n\times{1}$; The noise vector $\mathbold{\zeta}$ is unknown, random,
     $m b\times{1}$. The observation equation looks similar to the expression for the residual
     above. It's worthwhile to take a little time to examine the notations carefully and make sure
     that you have a good mental picture of the meanings of these notations. The
     observation equation looks tall in the typical, overdetermined case, where
     as the residual is usually equivalent to a scalar expression.

- $\mathbold{K}$ :: /Kalman gain/
     $=
     {\tilde{\mathbold{P}}}\,
     \mathbold{A}^\intercal\,
     {\mathbold{D}}^{-1}$ (proof
     sketch below).
     Non-random, $n\times{b}$.

- $\mathbold{D}$ :: /Kalman denominator/
     $=
     \mathbold{Z}+
     \mathbold{A}\,
     {\tilde{\mathbold{P}}}\,
     \mathbold{A}^\intercal$,
     or 
     $\mathbold{1}+
     \mathbold{A}\,
     {\tilde{\mathbold{P}}}\,
     \mathbold{A}^\intercal$
     de-dimensionalized.
     (proof sketch below). Non-random, \(b\times{b}\).

** Demonstration that Prior Covariance ${\tilde{\mathbold{P}}} = \tilde{\mathbold{A}}^{-2}$

The fact that the prior covariance, $\tilde{\mathbold{P}}$, equals the
the inverse square of
the stack of prior partials matrices (de-dimensionalized), $\tilde{\mathbold{A}}^{-2}$, is the secret
to Kalman's efficient, in fact constant, use of computer memory. The stack of
prior partials matrices $\tilde{\mathbold{A}}$ can be very tall and impractical
to store. But its square, $\tilde{\mathbold{A}}^{2}$ is only $n\times{n}$, and
its inverse square is also just $n\times{n}$. Kalman packs all statistical
information about the model into this small matrix of constant size, and
incrementally improves the statistics as observations accumulate, without
increasing the size of the matrix, and thus without increasing the amount of
computer memory needed to keep all important information. The Kalman filter is
/optimal/, meaning that the small covariance matrices keep all available
information. No other method would be able to squeeze more information out of
the observations and the model --- at least when the noise is Gaussian. A
rigorous optimality proof is out of scope for this paper, but the least-squares
derivation below contains the central idea: Kalman tracks the estimate and
covariance that minimize the sum of squared residuals. Kalman is optimal in the
sense that no other method would find a smaller sum of squared residuals.


*** Covariance of Any Random Vector Variable

The covariance of any random column vector $\mathbold{y}$ is defined as the
expectation value
$E
\left[
\mathbold{y}\,
\mathbold{y}^\intercal
\right]
=
E
\left[
({\mathbold{y}^\intercal})^2
\right]$
\noindent This is the expectation value of an outer product of a column vector
$\mathbold{y}$ and its transpose, $\mathbold{y}^\intercal$. Therefore, it is a
$q\times{q}$ matrix, where $q\times{1}$ is the dimensionality of $\mathbold{y}$.

*** Prior Estimate ${\tilde{\mathbold{x}}}$

One of our random variables is $\mathbold{x}$, the column \mbox{$n$-vector} of unknown
states. To calculate its estimate, assume we know the values of all $m$ past
partials ${\tilde{\mathbold{A}}}$ (tall, $m b\times{n}$) and observations
$\tilde{\mathbold{z}}$ (tall, $m b\times{1}$).

Relate $\mathbold{x}$ to the known observations ${\tilde{\mathbold{z}}}$ and the known
partials ${\tilde{\mathbold{A}}}$ through the normally distributed random noise column
vector $\mathbold{\zeta}$ and the /observation equation/:

#+BEGIN_LaTeX
\begin{equation}
\label{eqn:observation-equation}
{\tilde{\mathbold{z}}}={\tilde{\mathbold{A}}}\,\mathbold{x}+\mathbold{\zeta}
\end{equation}
#+END_LaTeX

*** Sum of Squared Residuals

Consider the
following /performance functional/, computed over the population of
$\mathbold{x}$.

#+BEGIN_LaTeX
\begin{equation*}
J(\mathbold{x})
\stackrel{\text{\tiny def}}{=}
\zeta^2=
\left(
{\tilde{\mathbold{z}}}-
{\tilde{\mathbold{A}}}\,
\mathbold{x}
\right)^2=
\left(
{\tilde{\mathbold{z}}}-
{\tilde{\mathbold{A}}}\,
\mathbold{x}
\right)^\intercal
\cdot
\left(
{\tilde{\mathbold{z}}}-
{\tilde{\mathbold{A}}}\,
\mathbold{x}
\right)
\end{equation*}
#+END_LaTeX

\noindent $J(\mathbold{x})$ is a scalar: the sum of squared residuals. A
/residual/ is a difference between an actual observation $\mathbold{z}$ and a
predicted observation $\mathbold{A}\,\mathbold{x}$. An /actual observation/
$\mathbold{z}$ is a known,
concrete \mbox{$b$-vector} of numbers, and the partials matrix $\mathbold{A}$
is a known, concrete \mbox{$(b\times n)$-matrix} of numbers
corresponding to that observation. The observation equation

- stacks all prior observations (known, concrete numbers) into
  $\tilde{\mathbold{z}}$
- stacks all prior 
  values of the partials matrix $\mathbold{A}$ into $\tilde{\mathbold{A}}$ (known,
  concrete numbers)
- multiplies by the unknown random state estimate
  $\mathbold{x}$ to get the (unknown, random)
  predicted observations ${\tilde{\mathbold{A}}}\,\mathbold{x}$
- finally adds some
  unknown random noise $\mathbold{\zeta}$ (column vector of height $m b$)

The performance functional collapses all that
information into a scalar random variable $J(\mathbold{x})$ with the same (Gaussian) distribution
as the noise $\mathbold{\zeta}$. Recall that any /random variable/ is, in fact,
always a
function, even if only the identity function, as when we say that $\mathbold{x}$
is a random variable. This is the standard nomenclature of probability and
statistics established by Kolmogorov, and it admittedly can be confusing.

The job of finding the optimal estimate of the state vector $\mathbold{x}$ is
the job of finding the concrete, numerical value of $\mathbold{x}$ that minimizes the
performance functional $J(\mathbold{x})$, which depends on all the known,
non-random, concrete numbers in $\tilde{\mathbold{z}}$ and $\tilde{\mathbold{A}}$.

To
find the $\mathbold{x}$ that minimizes $J(\mathbold{x})$, we could take the
classic, school approach of setting to zero the partial derivatives of
$J(\mathbold{x})$ with respect to $\mathbold{x}$ and solving the resulting
equations for $\mathbold{x}$. The following is an easier way. Multiply the
residuals across by the wide matrix ${\tilde{\mathbold{A}}}^\intercal$:

#+BEGIN_LaTeX
\begin{equation*}
{\tilde{\mathbold{A}}}^\intercal\,
{\tilde{\mathbold{z}}} - 
{\tilde{\mathbold{A}}}^2\,
\mathbold{x}
\end{equation*}
#+END_LaTeX

\noindent producing an \mbox{$n$-vector}, and then construct a
modified performance functional:

#+BEGIN_LaTeX
\begin{equation*}
J'(\mathbold{x})
\stackrel{\text{\tiny def}}{=}
\left(
{\tilde{\mathbold{A}}}^\intercal\,
{\tilde{\mathbold{z}}} -
{\tilde{\mathbold{A}}}^2\,
\mathbold{x}
\right)^2
=
\left(
{\tilde{\mathbold{A}}}^\intercal\,
{\tilde{\mathbold{z}}} -
{\tilde{\mathbold{A}}}^2\,
\mathbold{x}
\right)^\intercal
\cdot
\left(
{\tilde{\mathbold{A}}}^\intercal\,
{\tilde{\mathbold{z}}} -
{\tilde{\mathbold{A}}}^2\,
\mathbold{x}\right)
\end{equation*}
#+END_LaTeX

\noindent $J(\mathbold{x})$ is minimum with respect to $\mathbold{x}$ if and
only if (iff) $J'(\mathbold{x})$ is minimum (this assertion needs a rigorous
proof; as warned, we present only sketches in this paper). Because
$J'(\mathbold{x})$ is non-negative, when $J'(\mathbold{x})$ /can/ be zero, its
minimum /must/ be zero. $J'(\mathbold{x})$ is zero iff
${\tilde{\mathbold{A}}}^2$, an $n\times{n}$ square matrix, is invertible
(non-singular), in which case

#+BEGIN_LaTeX
\begin{equation*}
\mathbold{x}=
{\tilde{\mathbold{A}}}^{-2}\,
{\tilde{\mathbold{A}}}^\intercal\,
{\tilde{\mathbold{z}}}
\end{equation*}
#+END_LaTeX

\noindent produces that minimum value of $J'(\mathbold{x})$, because then

#+BEGIN_LaTeX
\begin{equation*}
{\tilde{\mathbold{A}}}^\intercal\,
{\tilde{\mathbold{z}}}=
{\tilde{\mathbold{A}}}^2\,
\mathbold{x}
\end{equation*}
#+END_LaTeX

We call such a solution for $\mathbold{x}$ the /least-squares estimate/ of
$\mathbold{x}$: the estimate of
$\mathbold{x}$ based on all prior observations.
From now on, we write it as ${\tilde{\mathbold{x}}}$

#+BEGIN_LaTeX
\begin{equation}
\label{eqn:least-squares-estimate}
\tilde{\mathbold{x}}
\stackrel{\text{\tiny def}}{=}
{\tilde{\mathbold{A}}}^{-2}
{\tilde{\mathbold{A}}}^\intercal
{\tilde{\mathbold{z}}} 
\end{equation}
#+END_LaTeX

With this solution, we get a new expression for the performance functional
$J(\mathbold{x})$ that is  useful below. First note that 

#+BEGIN_LaTeX
\begin{alignat}{6}
\notag
{\tilde{\mathbold{A}}}^2\,
{\tilde{\mathbold{A}}}^{-2}
&=
\mathbold{1}
&& \text{}
\\
\notag
{\tilde{\mathbold{A}}}^2\,
{\tilde{\mathbold{A}}}^{-2}
{\tilde{\mathbold{A}}}^\intercal
&=
{\tilde{\mathbold{A}}}^\intercal
&& 
\quad\text{Multiply on right by }\tilde{\mathbold{A}}^\intercal
\\
\notag
({\tilde{\mathbold{A}}}^\intercal\,
{\tilde{\mathbold{A}}})\,
{\tilde{\mathbold{A}}}^{-2}
{\tilde{\mathbold{A}}}^\intercal
&=
{\tilde{\mathbold{A}}}^\intercal
&&
\quad\text{Expand definition of }{\tilde{\mathbold{A}}}^2
\\
\label{eqn:aa2at-is-one}
\mathrm{therefore}\quad
{\tilde{\mathbold{A}}}\,
{\tilde{\mathbold{A}}}^{-2}\,
{\tilde{\mathbold{A}}}^\intercal
&=
\mathbold{1}
&&
\quad\text{Arbitrariness of }\tilde{\mathbold{A}}^\intercal\text{on left}
\end{alignat}
#+END_LaTeX



\noindent Equation \ref{eqn:aa2at-is-one} is another assertion that requires a
rigorous proof, out of scope for this paper of sketches. But, assuming it is
true, we have

#+BEGIN_LaTeX
\begin{alignat}{6}
\notag
J(\mathbold{x})
&=
\left(
{\tilde{\mathbold{z}}}-
{\tilde{\mathbold{A}}}\,
\mathbold{x}
\right)^\intercal
\cdot
\left(
{\tilde{\mathbold{z}}}-
{\tilde{\mathbold{A}}}\,
\mathbold{x}
\right)
\\
\notag
&=
\left(
{\tilde{\mathbold{z}}}-
{\tilde{\mathbold{A}}}\,
\mathbold{x}
\right)^\intercal
{\tilde{\mathbold{A}}}\,
{\tilde{\mathbold{A}}}^{-2}\,
{\tilde{\mathbold{A}}}^\intercal
\left(
{\tilde{\mathbold{z}}}-
{\tilde{\mathbold{A}}}\,
\mathbold{x}
\right)
&&
\quad\text{insert }\mathbold{1}\text{ from equation \ref{eqn:aa2at-is-one}}
\\
\notag
&=
\left(
{\tilde{\mathbold{z}}}-
{\tilde{\mathbold{A}}}\,
\mathbold{x}
\right)^\intercal
{\tilde{\mathbold{A}}}\,
({\tilde{\mathbold{A}}}^{-2}\,
{\tilde{\mathbold{A}}}^2)\,
{\tilde{\mathbold{A}}}^{-2}\,
{\tilde{\mathbold{A}}}^\intercal
\left(
{\tilde{\mathbold{z}}}-
{\tilde{\mathbold{A}}}\,
\mathbold{x}
\right)
&&
\quad\text{insert }\mathbold{1} = {\tilde{\mathbold{A}}}^{-2}\,{\tilde{\mathbold{A}}}^{2}
\\
\notag
&=
\left[
\left(
{\tilde{\mathbold{z}}}-
{\tilde{\mathbold{A}}}\,
\mathbold{x}
\right)^\intercal
{\tilde{\mathbold{A}}}\,
{\tilde{\mathbold{A}}}^{-2}
\right]
{\tilde{\mathbold{A}}}^2
\left[
{\tilde{\mathbold{A}}}^{-2}\,
{\tilde{\mathbold{A}}}^\intercal
\left(
{\tilde{\mathbold{z}}}-
{\tilde{\mathbold{A}}}\,
\mathbold{x}
\right)
\right]
&&
\quad\text{Regroup}
\\
\notag
&=
\left[
{\tilde{\mathbold{A}}}^{-2}\,
{\tilde{\mathbold{A}}}^\intercal
\left(
{\tilde{\mathbold{z}}}-
{\tilde{\mathbold{A}}}\,
\mathbold{x}
\right)
\right]^\intercal
{\tilde{\mathbold{A}}}^2
\left[
{\tilde{\mathbold{A}}}^{-2}\,
{\tilde{\mathbold{A}}}^\intercal
\left(
{\tilde{\mathbold{z}}}-
{\tilde{\mathbold{A}}}\,
\mathbold{x}
\right)
\right]
&&
\quad\text{Symmetry of $\tilde{\mathbold{A}}$ and $\tilde{\mathbold{A}}^{-2}$}
\\
\label{eqn:performance-functional-reformed}
&=
(\tilde{\mathbold{x}}-\mathbold{x})^\intercal\,
{\tilde{\mathbold{A}}^2}\,
(\tilde{\mathbold{x}}-\mathbold{x})
&&
\quad\text{Definition of }{\tilde{\mathbold{x}}}\text{ from equation \ref{eqn:least-squares-estimate}}
\end{alignat}
#+END_LaTeX

\noindent
This has
physical dimensions $\mathcal{Z}^2$ where $\mathcal{Z}$ are the physical
dimensions of the observations $\mathbold{z}$.

*** Prior Covariance $\tilde{\mathbold{P}}$

We now want the covariance of the residuals between
our least-squares estimate $\tilde{\mathbold{x}}$ and the random vector
$\mathbold{x}$:

#+BEGIN_LaTeX
\begin{align}
\label{eqn:covariance-of-x}
\tilde{\mathbold{P}}
\stackrel{\text{\tiny def}}{=}
E
\left[
(\tilde{\mathbold{x}}-\mathbold{x})
(\tilde{\mathbold{x}}-\mathbold{x})^\intercal
\right]
\end{align}
#+END_LaTeX

\noindent  Get $\tilde{\mathbold{x}}-\mathbold{x}$
from the observations and partials at hand as follows:

#+BEGIN_LaTeX
\begin{alignat}{6}
\notag
{\tilde{\mathbold{z}}}
&=
{\tilde{\mathbold{A}}}\,
\mathbold{x} + 
\mathbold{\zeta}
&&
\quad\text{the observation equation, Equation \ref{eqn:observation-equation}}
\\
\notag
{\tilde{\mathbold{A}}}^{-2}\,
{\tilde{\mathbold{A}}}^\intercal\,
{\tilde{\mathbold{z}}}
&=
\mathbold{x} + 
{\tilde{\mathbold{A}}}^{-2}\,
{\tilde{\mathbold{A}}}^\intercal\,
\mathbold{\zeta}
&&
\quad\text{Multiply on left by }{\tilde{\mathbold{A}}}^{-2}\,\tilde{\mathbold{A}}^\intercal
\\
\notag
\tilde{\mathbold{x}}
&=
\mathbold{x} +
{\tilde{\mathbold{A}}}^{-2}\,
{\tilde{\mathbold{A}}}^\intercal\,
\mathbold{\zeta}
&&
\quad\text{Definition of }{\tilde{\mathbold{x}}}\text{ from equation \ref{eqn:least-squares-estimate}}
\\
\notag
\text{therefore}\quad
\tilde{\mathbold{x}} -
\mathbold{x} &=
{\tilde{\mathbold{A}}}^{-2}
{\tilde{\mathbold{A}}}^\intercal
\mathbold{\zeta}
\end{alignat}
#+END_LaTeX

\noindent
Now rewrite equation \ref{eqn:covariance-of-x}, the definition of the prior
covariance $\tilde{\mathbold{P}}$:

#+BEGIN_LaTeX
\begin{align}
\notag
E
\left[
(\tilde{\mathbold{x}}-\mathbold{x})
(\tilde{\mathbold{x}}-\mathbold{x})^\intercal
\right] &=
E
\left[
{\tilde{\mathbold{A}}}^{-2}
{\tilde{\mathbold{A}}}^\intercal
\mathbold{\zeta}\,
\mathbold{\zeta}^\intercal
({\tilde{\mathbold{A}}}^{-2}
{\tilde{\mathbold{A}}}^\intercal
\mathbold{\zeta})^\intercal
\right] \\
\label{eqn:almost-final-covariance}
&=
{\tilde{\mathbold{A}}}^{-2}
{\tilde{\mathbold{A}}}^\intercal\,
E\left[
\mathbold{\zeta}\,
\mathbold{\zeta}^\intercal
\right]
({\tilde{\mathbold{A}}}^{-2}
{\tilde{\mathbold{A}}}^\intercal)^\intercal
\end{align}
#+END_LaTeX

\noindent We can collapse the expectation value inwards because the stack of
observation partials $\tilde{\mathbold{A}}$ is a matrix of concrete, non-random
numbers. 

Noise $\mathbold{\zeta}$ is Gaussian, normal, with diagonal covariance
matrix $\mathbold{Z}$, by hypothesis. Equation \ref{eqn:almost-final-covariance}
becomes

#+BEGIN_LaTeX
\begin{align} 
\notag
\tilde{\mathbold{P}} =
{\tilde{\mathbold{A}}}^{-2}
{\tilde{\mathbold{A}}}^\intercal\,
E\left[
\mathbold{\zeta}\,\mathbold{\zeta}^\intercal
\right]
({\tilde{\mathbold{A}}}^{-2}
{\tilde{\mathbold{A}}}^\intercal)^\intercal 
&= 
\notag
{\tilde{\mathbold{A}}}^{-2}
{\tilde{\mathbold{A}}}^\intercal\,
\mathbold{Z}\,
({\tilde{\mathbold{A}}}^{-2}
{\tilde{\mathbold{A}}}^\intercal)^\intercal 
\\
&= 
\notag
{\tilde{\mathbold{A}}}^{-2}
{\tilde{\mathbold{A}}}^\intercal\,
\mathbold{Z}\,
{\tilde{\mathbold{A}}}
({\tilde{\mathbold{A}}}^{-2})^\intercal
\\
&= 
\label{eqn:prior-covariance-convenient-form}
{\tilde{\mathbold{A}}}^{-2}
{\tilde{\mathbold{A}}}^\intercal\,
\mathbold{Z}\,
{\tilde{\mathbold{A}}}
({\tilde{\mathbold{A}}}^{-2})
\end{align}
#+END_LaTeX

\noindent because $\tilde{\mathbold{A}}^{-2}$ is symmetric.
At this point, no further simplification is possible, in general, because
$\mathbold{Z}$ is $b\times b$ and can only be sandwiched between
${\tilde{\mathbold{A}}}^\intercal$, $n\times b$, and 
${\tilde{\mathbold{A}}}$, $b\times n$. However, we can greatly simplify this and
all subsequent computations by de-dimensionalizing. There are numerical
benefits, as well, to be discussed in the next section.

*** De-Dimensionalizing the Observation Equation

Fully spelled out, and in the general case of \mbox{$b$-vector} observations
$\mathbold{z}$, one block of height $b$ of the observation equation is

#+BEGIN_LaTeX
\begin{equation*}
\left(
\begin{array}{c}
 z_1 \\
 z_2 \\
 \vdots  \\
 z_b \\
\end{array}
\right)=\left(
\begin{array}{cccc}
 A_{11} & A_{12} & \cdots  & A_{1 n} \\
 A_{21} & A_{22} & \cdots  & A_{2 n} \\
 \vdots  & \vdots  & \ddots & \vdots  \\
 A_{\text{b1}} & A_{\text{b2}} & \cdots  & A_{b n} \\
\end{array}
\right) \left(
\begin{array}{c}
 x_1 \\
 x_2 \\
 \vdots  \\
 x_n \\
\end{array}
\right)+\left(
\begin{array}{c}
 \zeta _1 \\
 \zeta _2 \\
 \vdots  \\
 \zeta _b \\
\end{array}
\right)
\end{equation*}
#+END_LaTeX

If we divide each row $i$ by the standard deviation $\sigma_{z_i}$ of the \mbox{$i$-th}
component $z_i$ of the observation $\mathbold{z}$, we get

#+BEGIN_LaTeX
\begin{equation}
\label{eqn:de-dimensionalized-observation-equation}
\left(
\begin{array}{c}
 \frac{z_1}{\sigma _{z_1}} \\
 \frac{z_2}{\sigma _{z_2}} \\
 \vdots  \\
 \frac{z_b}{\sigma _{z_b}} \\
\end{array}
\right)=\left(
\begin{array}{cccc}
 \frac{A_{11}}{\sigma _{z_1}} & \frac{A_{12}}{\sigma
   _{z_1}} & \cdots  & \frac{A_{1 n}}{\sigma _{z_1}} \\
 \frac{A_{21}}{\sigma _{z_2}} & \frac{A_{22}}{\sigma
   _{z_2}} & \cdots  & \frac{A_{2 n}}{\sigma _{z_2}} \\
 \vdots  & \vdots  & \ddots & \vdots  \\
 \frac{A_{\text{b1}}}{\sigma _{z_b}} &
   \frac{A_{\text{b2}}}{\sigma _{z_b}} & \cdots  &
   \frac{A_{b n}}{\sigma _{z_b}} \\
\end{array}
\right) \left(
\begin{array}{c}
 x_1 \\
 x_2 \\
 \vdots  \\
 x_n \\
\end{array}
\right)+\left(
\begin{array}{c}
 \frac{\zeta _1}{\sigma _{z_1}} \\
 \frac{\zeta _2}{\sigma _{z_2}} \\
 \vdots  \\
 \frac{\zeta _b}{\sigma _{z_b}} \\
\end{array}
\right)
\end{equation}
#+END_LaTeX

The covariance of the noise $\mathbold{\zeta}$, so normalized, is non-dimensional
unity and equation \ref{eqn:prior-covariance-convenient-form} collapses
completely to just

#+BEGIN_LaTeX
\begin{equation}
\label{eqn:prior-covariance-most-convenient-form}
\tilde{\mathbold{P}}={\tilde{\mathbold{A}}}^{-2}
\end{equation}
#+END_LaTeX

\noindent and the estimate of the priors, equation
\ref{eqn:least-squares-estimate} now becomes

#+BEGIN_LaTeX
\begin{equation}
\label{eqn:estimate-of-the-priors}
\tilde{\mathbold{x}}
\stackrel{\text{\tiny def}}{=}
\tilde{\mathbold{P}}\,
{\tilde{\mathbold{A}}}^\intercal
{\tilde{\mathbold{z}}} 
\end{equation}
#+END_LaTeX


This is remarkable. All information about the covariance of the noise is pulled
into the (new, normalized) observation partials. 

I remember, when working in the early 1980's at the Deep Space Network at JPL on
direct measurement of tectonic drift,[fn:jplg] one difficulty was the wide
disparity between uncertainties of horizontal measurments (right ascension and
declination) and uncertainties in range. For instance, we knew the RA-dec
position of the centroid of Saturn within 75 meters but its distance to no
better than a million kilometers. That's a disparity of seven orders of
magnitude (the situation is greatly improved, now, due to the accumulation of
range data for multiple spacecraft coupled with decades of orbital
mechanics[fn:folk]). At the time, this meant that we had to deal with error
ellipsoids that were long, thin needles, covariance matrices with components
differing by up to fourteen orders of magnitude. That's not practical with
floating-point computer arithmetic. One mitigation was de-dimensionalizing or
normalizing, as described here, which brings the uncertainties of all components
of an observation into the same numerical range, near unity. Another mitigation
was Square Root Information Filtering (SRIF), the subject of another paper in
this series.

In any event, for all subsequent calculations in this paper, we assume that the
observation equation has been normalized and that $\mathbold{Z}=\mathbold{1}$. 


** Posterior Estimate $\hat{\mathbold{x}}$ and Covariance $\hat{\mathbold{P}}$

To effect incremental updates of $\mathbold{x}$ and $\mathbold{P}$, we need the
posterior estimate $\hat{\mathbold{x}}$ and covariance $\hat{\mathbold{P}}$ in
terms of the priors $\tilde{\mathbold{x}}$, $\tilde{\mathbold{P}}$, and the new
partials $\mathbold{A}$ and new observation $\mathbold{z}$, all  of which are
matrices of known, concrete, non-random numbers. This is exactly what our
/kalmanStatic/ function from equation \ref{eqn:kalman-cume-definition} does, of course,
in functional form.  We derive the posteriors from scratch to seek
opportunities to define $\mathbold{K}$ and $\mathbold{D}$ and to radically shorten
the expressions. 

First, define a new performance functional $J_1(\mathbold{x})$ as the sum of the 
performance of the priors $\tilde{J}(\mathbold{x})$ from equation
\ref{eqn:performance-functional-reformed}, now written with tildes overhead,
and a new term
$J_2(\mathbold{x})$ for the
performance of the new data:

#+BEGIN_LaTeX
\begin{alignat}{6}
J_1(\mathbold{x})
& \stackrel{\text{\tiny def}}{=}
{\tilde{J}}(\mathbold{x}) +
J_2(\mathbold{x})
\\
\notag
{\tilde{J}}(\mathbold{x})
&\stackrel{\text{\tiny def}}{=}
(\tilde{\mathbold{x}}-\mathbold{x})^\intercal\,
{\tilde{\mathbold{A}}^2}\,
(\tilde{\mathbold{x}}-\mathbold{x})
&&
\quad\text{Equation \ref{eqn:performance-functional-reformed}}
\\
\label{eqn:performance-of-new-data}
J_2(\mathbold{x})
&\stackrel{\text{\tiny def}}{=}
\left(
\mathbold{z}-
\mathbold{A}\,
\mathbold{x}
\right)^2
\\
\notag
&=
\left(
\mathbold{z}-
\mathbold{A}\,
\mathbold{x}
\right)^\intercal
\cdot
\left(
\mathbold{z}-
\mathbold{A}\,
\mathbold{x}
\right)
\\
\notag
&=
\mathbold{z}^2 -
\mathbold{z}^\intercal\,
\mathbold{A}\,
\mathbold{x} -
\mathbold{z}\,
\mathbold{x}^\intercal\,
\mathbold{A}^\intercal +
\left(
\mathbold{A}\,
\mathbold{x}
\right)^2
\\
\notag
&=
\mathbold{z}^2 -
2\,
\mathbold{z}^\intercal\,
\mathbold{A}\,
\mathbold{x} +
\left(
\mathbold{A}\,
\mathbold{x}
\right)^2
\end{alignat}
#+END_LaTeX

This time, I don't have a handy trick for minimizing the performance functional.
Let's find the minimizing $\mathbold{x}$ the classic way: by solving
$d\,J_1(\mathbold{x})/d\,\mathbold{x}=0$. The usual way to write a vector
derivative is with the /nabla/ operator $\nabla$, which produces /gradient/
vectors from scalar functions.

#+BEGIN_LaTeX
\begin{align*}
\nabla{}\,f(\mathbold{x}) &\stackrel{\text{\tiny def}}{=}
\begin{bmatrix}
df(\mathbold{x})/dx_0\\
df(\mathbold{x})/dx_1\\
\vdots\\
df(\mathbold{x})/dx_{n-1}
\end{bmatrix}
\end{align*}
#+END_LaTeX

The particular scalar function we're differentiating is, of course, the new
performance functional
$J_1(\mathbold{x})=
{\tilde{J}}(\mathbold{x})+
J_2(\mathbold{x})$. Because
${\tilde{\mathbold{A}}^2}$ is symmetric,

#+BEGIN_LaTeX
\begin{align*}
\nabla{}\,
{\tilde{J}}(\mathbold{x}) &=
\nabla{}
\left(
(\tilde{\mathbold{x}}-\mathbold{x})^\intercal\,
{\tilde{\mathbold{A}}^2}\,
(\tilde{\mathbold{x}}-\mathbold{x})
\right) \\ &=
-2\,
{\tilde{\mathbold{A}}^2}\,
(\tilde{\mathbold{x}}-\mathbold{x})
\end{align*}
#+END_LaTeX

\noindent an \mbox{$n$-vector}, and we similarly compute the gradient of
$J_2(\mathbold{x})$, which contains the new observation and partials:

#+BEGIN_LaTeX
\begin{align*}
\nabla\,
J_2(\mathbold{x})
&=
\nabla
\left(
\mathbold{z}^2 -
2\,
\mathbold{z}^\intercal\,
\mathbold{A}\,
\mathbold{x} +
\left(
\mathbold{A}\,
\mathbold{x}
\right)^2
\right)
\\
&=
2\,
\mathbold{A}^\intercal
\left(
\mathbold{A}\,
\mathbold{x} -
\mathbold{z}
\right)
\\
&=
2\,
\left(
\mathbold{A}^2\,
\mathbold{x}-
\mathbold{A}^\intercal\,
\mathbold{z}
\right)
\end{align*}
#+END_LaTeX

\noindent another \mbox{$n$-vector}. We can solve the resulting equation for
$\mathbold{x}$ on sight, writing the new solution --- the new estimate ---
with an overhat. Be aware that
that $\mathbold{A}$ is a wide matrix, in fact   an \mbox{$n$-row} when $b=1$, a
common case, and 
$\mathbold{A}^2$ is thus an outer product and an $n\times{n}$ matrix.


#+BEGIN_LaTeX
\begin{align}
\notag
\nabla{}\,
J_1(\mathbold{x}) 
&= 
\nabla{}\,
{\tilde{J}}
(\mathbold{x}) + 
\nabla{}\,
J_2(\mathbold{x}) 
= 0
\\
\notag
&=
{\tilde{\mathbold{A}}}^2\,
\mathbold{x} -
{\tilde{\mathbold{A}}}^2\,
{\tilde{\mathbold{x}}} +
\mathbold{A}^2\,
\mathbold{x} - 
\mathbold{A}^\intercal{}\,
\mathbold{z}
\\
\label{eqn:def-of-posterior-estimate}
&
\text{if and only if}\quad
\mathbold{x}=\hat{\mathbold{x}}
\stackrel{\text{\tiny def}}{=}
\left(
{\tilde{\mathbold{A}}}^2 + 
\mathbold{A}^2
\right)^{-1}
\cdot
\left(
\mathbold{A}^\intercal\,
\mathbold{z} + 
{\tilde{\mathbold{A}}}^2\,
{\tilde{\mathbold{x}}}
\right)
\end{align}
#+END_LaTeX

Look how pretty this is. Equation \ref{eqn:estimate-of-the-priors} for the
priors gave us the form
$\tilde{\mathbold{x}}= \tilde{\mathbold{P}}\,
\tilde{\mathbold{A}}^\intercal\,\tilde{\mathbold{z}}$, a covariance 
$\tilde{\mathbold{P}}$
times  the prior observations 
$\tilde{\mathbold{z}}$
scaled by the prior partials, transposed, 
$\tilde{\mathbold{A}}^\intercal$. 
The new estimate $\hat{\mathbold{x}}$ has exactly
the same form if we regard the first matrix factor
$\left({\tilde{\mathbold{A}}}^2 + \mathbold{A}^2 \right)^{-1}$ 
as a  covariance
$\hat{\mathbold{P}}$  and if
we regard /all/ the priors ${\tilde{\mathbold{A}}}^2\,{\tilde{\mathbold{x}}}$ as a /single/
scaled observation
to add to the current scaled observation $\mathbold{A}^\intercal\,\mathbold{z}$.
We may regard ${\tilde{\mathbold{A}}^2}\,\tilde{\mathbold{x}}$ as a scaled
observation because
equations
\ref{eqn:prior-covariance-most-convenient-form}
and
\ref{eqn:estimate-of-the-priors}
imply that
${\tilde{\mathbold{A}}^\intercal}\,\tilde{\mathbold{z}}={\tilde{\mathbold{A}}^2}\,\tilde{\mathbold{x}}$. 
We may view the second term above,
$\mathbold{A}^\intercal\,
\mathbold{z} + 
{\tilde{\mathbold{A}}}^2\,
{\tilde{\mathbold{x}}}$, 
as
$\mathbold{A}^\intercal\,
\mathbold{z} + 
{\tilde{\mathbold{A}}}^\intercal\,
{\tilde{\mathbold{z}}}$.

*** Posterior estimate, $\hat{\mathbold{x}}$

We must wrangle 
equation
\ref{eqn:kalman-update-equation}
from
equation
\ref{eqn:def-of-posterior-estimate}.
Equation 
\ref{eqn:kalman-update-equation}
is the recurrence we want,
namely 
$\hat{\mathbold{x}}=\tilde{\mathbold{x}}+\mathbold{K}(\mathbold{z}-\mathbold{A}\,\tilde{\mathbold{x}})$,
and equation
\ref{eqn:def-of-posterior-estimate}
is the recurrence we have, namely\\
\(
\hat{\mathbold{x}}
=
\left(
{\tilde{\mathbold{A}}}^2 + 
\mathbold{A}^2
\right)^{-1}\,
\left(
\mathbold{A}^\intercal\,
\mathbold{z} + 
{\tilde{\mathbold{A}}}^2\,
{\tilde{\mathbold{x}}}
\right)
\).

First, formally define the new, posterior covariance.

#+BEGIN_LaTeX
\begin{equation}
\label{eqn:new-p-hat-definition}
{\hat{\mathbold{P}}}
\stackrel{\text{\tiny def}}{=}
\left(
{\tilde{\mathbold{A}}}^2 + \mathbold{A}^2
\right)^{-1}
\end{equation}
#+END_LaTeX

\noindent Now write 
equation
\ref{eqn:def-of-posterior-estimate}
as

#+BEGIN_LaTeX
\begin{align*}
\hat{\mathbold{x}}
&=
\hat{\mathbold{P}}\,
\left(
\mathbold{A}^\intercal\,
\mathbold{z} + 
{\tilde{\mathbold{A}}}^2\,
{\tilde{\mathbold{x}}}
\right)
\\
\\
&=
\hat{\mathbold{P}}\,\mathbold{A}^\intercal\,
\mathbold{z}
+
\hat{\mathbold{P}}\,
\tilde{\mathbold{A}}^2\,\tilde{\mathbold{x}}
\\
\end{align*}
#+END_LaTeX


The form above strongly suggests that we define

#+BEGIN_LaTeX
\begin{equation}
\label{eqn:kalman-gain-new-definition}
\mathbold{K}
\stackrel{\text{\tiny def}}{=}
{\hat{\mathbold{P}}}\,
\mathbold{A}^\intercal
\end{equation}
#+END_LaTeX

\noindent yielding

#+BEGIN_LaTeX
\begin{align}
\label{eqn:first-part-of-gain-proof}
\hat{\mathbold{x}}
&=
\mathbold{K}\,
\mathbold{z}
+
{\hat{\mathbold{P}}}\,
\tilde{\mathbold{A}}^2\,\tilde{\mathbold{x}}
\end{align}
#+END_LaTeX

\noindent Now, to get the recurrence we want

#+BEGIN_LaTeX
\begin{align}
\notag
\hat{\mathbold{x}}
&=
\tilde{\mathbold{x}}+
\mathbold{K}
\left(
\mathbold{z}-
\mathbold{A}\,
{\tilde{\mathbold{x}}}
\right)
\\
\label{eqn:second-part-of-gain-proof}
&=
\tilde{\mathbold{x}}+
\mathbold{K}\,
\mathbold{z}-
\mathbold{K}\,
\mathbold{A}\,
{\tilde{\mathbold{x}}}
\end{align}
#+END_LaTeX

\noindent we need only set equation \ref{eqn:first-part-of-gain-proof} equal to
equation \ref{eqn:second-part-of-gain-proof}.  Cancelling terms and rearranging,
we get

#+BEGIN_LaTeX
\begin{align}
\label{eqn:recurrence-to-prove}
(\mathbold{1}-\mathbold{K}\,\mathbold{A})\,
\tilde{\mathbold{x}}
&=
{\hat{\mathbold{P}}}\,
\tilde{\mathbold{A}}^2\,\tilde{\mathbold{x}} 
=
{\hat{\mathbold{P}}}\,
\tilde{\mathbold{P}}^{-1}\,\tilde{\mathbold{x}} 
\end{align}
#+END_LaTeX

\noindent by definition of the prior covariance, equation
\ref{eqn:prior-covariance-most-convenient-form}. For arbitrary
$\tilde{\mathbold{x}}$, this will be true if 

#+BEGIN_LaTeX
\begin{align*}
(\mathbold{1}-\mathbold{K}\,\mathbold{A})
&=
{\hat{\mathbold{P}}}\,
\tilde{\mathbold{P}}^{-1}
\end{align*}
#+END_LaTeX

\noindent Rearrange and right-multiply by $\tilde{\mathbold{P}}$ to get

#+BEGIN_LaTeX
\begin{align}
\label{eqn:p-is-l-p}
\hat{\mathbold{P}}
&=\left(
\mathbold{1}-
\mathbold{K}\,
\mathbold{A}
\right)\,
\tilde{\mathbold{P}}
=
\hat{\mathbold{P}}\,{\tilde{\mathbold{A}}}^2\,\tilde{\mathbold{P}}
\end{align}
#+END_LaTeX

\noindent showing that equations \ref{eqn:recurrence-to-prove} and
\ref{eqn:kalman-update-equation} are just alternative expressions for the same
thing.


Let's write this more compactly

#+BEGIN_LaTeX
\begin{align}
\label{eqn:derivation-of-p-is-l-p}
{\hat{\mathbold{P}}} &=
\mathbold{L}\,
{\tilde{\mathbold{P}}}
\end{align}
#+END_LaTeX

\noindent where

#+BEGIN_LaTeX
\begin{equation}
\label{eqn:definition-of-l}
\mathbold{L}\stackrel{\text{\tiny def}}{=}
(\mathbold{1}-
\mathbold{K}\,
\mathbold{A})
=
\hat{\mathbold{P}}\,{\tilde{\mathbold{A}}}^2
\end{equation}
#+END_LaTeX

\noindent and we have
one of the three equivalent recurrences
for the posterior covariance
from the first paper in this series

#+BEGIN_LaTeX
\begin{equation}
{{\mathbold{P}}} \leftarrow
\mathbold{L}\,
{{\mathbold{P}}}
\end{equation}
#+END_LaTeX

*** A Gain Matrix $\mathbold{K}$ We Can Actually Compute



Of course, the gain matrix $\mathbold{K}$ is formally defined in terms of the
posterior covariance, that is, as $\hat{\mathbold{P}}\,\mathbold{A}^\intercal$,
but we don't have the posterior covariance $\hat{\mathbold{P}}$ by equation
\ref{eqn:p-is-l-p} until we have the gain matrix $\mathbold{K}$. To get out of
this fix, we note that 

#+BEGIN_LaTeX
\begin{equation*}
\mathbold{K}
=
\hat{\mathbold{P}}\,
\mathbold{A}^\intercal
=
\mathbold{L}\,
\tilde{\mathbold{P}}\,
\mathbold{A}^\intercal
=
(\mathbold{1}-
\mathbold{K}\,
\mathbold{A})\,
\tilde{\mathbold{P}}\,
\mathbold{A}^\intercal
\end{equation*}
#+END_LaTeX

\noindent and solve for $\mathbold{K}$:

#+BEGIN_LaTeX
\begin{align}
\notag
\mathbold{K}
&=
\tilde{\mathbold{P}}\,
\mathbold{A}^\intercal
-
\mathbold{K}\,
\mathbold{A}\,
\tilde{\mathbold{P}}\,
\mathbold{A}^\intercal
\\
\notag
\mathbold{K}\,
(\mathbold{1}+
\mathbold{A}\,
\tilde{\mathbold{P}}\,
\mathbold{A}^\intercal)
&=
\tilde{\mathbold{P}}\,
\mathbold{A}^\intercal
\\
\label{eqn:for-k}
\mathbold{K}
&=
\tilde{\mathbold{P}}\,
\mathbold{A}^\intercal\,
(\mathbold{1}+
\mathbold{A}\,
\tilde{\mathbold{P}}\,
\mathbold{A}^\intercal)^{-1}
\end{align}
#+END_LaTeX

Defining the Kalman denominator matrix $\mathbold{D}$ as follows:

#+BEGIN_LaTeX
\begin{equation}
\label{eqn:definition-of-d}
\mathbold{D}
\stackrel{\text{\tiny def}}{=}
\mathbold{1}+
\mathbold{A}\,
\tilde{\mathbold{P}}\,
\mathbold{A}^\intercal
\end{equation}
#+END_LaTeX

\noindent we finally get a form for the Kalman gain matrix $\mathbold{K}$
entirely in terms of priors and the new observation partials (sometimes called
the /innovation/):

#+BEGIN_LaTeX
\begin{align}
\label{eqn:kalman-gain-definition-2}
\mathbold{K}
&=
\tilde{\mathbold{P}}\,
\mathbold{A}^\intercal\,
\mathbold{D}^{-1} 
\\
\label{eqn:kalman-denominator-definition-2}
\text{where}\quad
\mathbold{D}
&= \mathbold{1} +
\mathbold{A}\,
\mathbold{P}\,
\mathbold{A}^\intercal
\end{align}
#+END_LaTeX

\noindent These are almost the same as the original definitions, equations
\ref{eqn:kalman-gain-definition} and \ref{eqn:kalman-denominator-definition},
which were written in dimensional form. We leave it to the reader to show that
the dimensional form for $\mathbold{D}$ is
$\mathbold{Z}+
\mathbold{A}\,
\mathbold{P}\,
\mathbold{A}^\intercal$.

*** Two More Recurrences

There remain
two more recurrences to derive, namely

#+BEGIN_LaTeX
\begin{equation}
\label{eqn:p-is-lplt-plus-kzkt}
\mathbold{P}\leftarrow
\mathbold{L}\,
\mathbold{P}\,
\mathbold{L}^\intercal +
\mathbold{K}\,
\mathbold{Z}\,
\mathbold{K}^\intercal
\end{equation}
#+END_LaTeX

\noindent and the canonical form,

#+BEGIN_LaTeX
\begin{equation}
\label{eqn:p-is-p-minus-kdkt}
\mathbold{P}\leftarrow
\mathbold{P} -
\mathbold{K}\,
\mathbold{D}\,
\mathbold{K}^\intercal
\end{equation}
#+END_LaTeX

*** Minimizing $J_1({\mathbold{x}})$

The posterior covariance is, from the statistical viewpoint,

#+BEGIN_LaTeX
\begin{equation*}
{\hat{\mathbold{P}}} =
E
\left[
({\hat{\mathbold{x}}}-\mathbold{x})
({\hat{\mathbold{x}}}-\mathbold{x})^\intercal
\right]
\end{equation*}
#+END_LaTeX

\noindent Get our new expression for ${\hat{\mathbold{x}}}$:

#+BEGIN_LaTeX
\begin{equation*}
{\hat{\mathbold{x}}} =
{\tilde{\mathbold{x}}}+
\mathbold{K}\,
(\mathbold{z}-
\mathbold{A}\,
{\tilde{\mathbold{x}}}) =
\mathbold{K}\,
\mathbold{z} +
\mathbold{L}\,
{\tilde{\mathbold{x}}}
\end{equation*}
#+END_LaTeX

\noindent where, again

#+BEGIN_LaTeX
\begin{equation*}
\mathbold{L}
=
(\mathbold{1}-
\mathbold{K}\,
\mathbold{A})
=
{\hat{\mathbold{P}}}\,
{\tilde{\mathbold{A}}}^2
\end{equation*}
#+END_LaTeX

\noindent
Remembering the observation equation
(\ref{eqn:observation-equation}), write a single instance of it
$\mathbold{z} =
\mathbold{A}\,
\mathbold{x}+
\mathbold{\zeta}$ and find

#+BEGIN_LaTeX
\begin{align}
\notag
{\hat{\mathbold{x}}}
&=
\mathbold{K}\,
\mathbold{A}\,
\mathbold{x} +
\mathbold{K}\,
\mathbold{\zeta} +
\mathbold{L}\,
{\tilde{\mathbold{x}}}
\\
\notag
&=
\left(
\mathbold{1}-
\mathbold{L}
\right)\,
\mathbold{x} +
\mathbold{K}\,
\mathbold{\zeta} +
\mathbold{L}\,
{\tilde{\mathbold{x}}}
\end{align}
#+END_LaTeX

\noindent implying that
\(
\left(
{\hat{\mathbold{x}}}-
\mathbold{x}
\right)=
\mathbold{L}\,
\left(
{\tilde{\mathbold{x}}}-
\mathbold{x}
\right) +
\mathbold{K}\,
\mathbold{\zeta}
\).

Remembering that
$E
\left[
\mathbold{\zeta}
\right]=\mathbold{0}$, 
$E
\left[
\mathbold{\zeta}\,
\mathbold{\zeta}^\intercal
\right]=\mathbold{Z}$, glibly re-dimensionalizing and skipping
intermediate steps, we find that 

#+BEGIN_LaTeX
\begin{equation}
{\hat{\mathbold{P}}} = 
\mathbold{L}\,
{\tilde{\mathbold{P}}}\,
\mathbold{L}^\intercal + 
\mathbold{K}\,
\mathbold{Z}\,
\mathbold{K}^\intercal
\end{equation}
#+END_LaTeX

\noindent We leave it to the reader to check, with reference to equations
\ref{eqn:dimensional-breakdown}, that the physical dimensions work out. This
completes the derivation of the recurrence equation \ref{eqn:p-is-lplt-plus-kzkt}. 

The last form,
$\hat{\mathbold{P}}
=
\tilde{\mathbold{P}}-
\mathbold{K}\,
\mathbold{D}\,
\mathbold{K}^\intercal$,
is easy to show from what we already know, that 
$\hat{\mathbold{P}}
=
\mathbold{L}\,
\tilde{\mathbold{P}}
=
(\mathbold{1}-
\mathbold{K}\,
\mathbold{A})\,
\tilde{\mathbold{P}}$.
We just need to show that 
$\mathbold{K}\,
\mathbold{A}\,
\tilde{\mathbold{P}} 
=
\mathbold{K}\,
\mathbold{D}\,
\mathbold{K}^\intercal$. 
Substitute 
$\mathbold{D}^{-\intercal}\,
\mathbold{A}\,
\tilde{\mathbold{P}}^\intercal$
for $\mathbold{K}^\intercal$ by transposing
equation \ref{eqn:kalman-gain-definition-2}.
Note that for square matrices, the inverse of the
transpose is the transpose of the inverse. Therefore
$\mathbold{D}^{-\intercal}
= \mathbold{D}^{-1}$ because $\mathbold{D}$ is symmetric. Likewise 
$\tilde{\mathbold{P}}^\intercal=\tilde{\mathbold{P}}$.  The result follows:

#+BEGIN_LaTeX
\begin{equation*}
\mathbold{K}\,
\mathbold{D}\,
\mathbold{K}^\intercal
=
\mathbold{K}\,
\mathbold{D}\,
\mathbold{D}^{-\intercal}\,
\mathbold{A}
\tilde{\mathbold{P}}
=
\mathbold{K}\,
\mathbold{A}
\tilde{\mathbold{P}}
\end{equation*}
#+END_LaTeX



* Concluding Remarks

These derivations are helpful for gaining intuition into the underlying
statistics and dimensional structures of the Kalman filter and its many
variants. They are a bit involved, but it is worthwhile to ingest these
fundamentals, especially for those who need to research new filters and
applications. For more rigorous proofs built on a Bayesian perspective, see
Bar-Shalom.[fn:bars] For more careful dimensional analysis of the present
derivations, see part 6 of this series.[fn:klf6]

[fn:affn] https://en.wikipedia.org/wiki/Affine_transformation
[fn:bars] Bar-Shalom, Yaakov, /et al/. Estimation with applications to tracking and navigation. New York: Wiley, 2001.
[fn:bier] http://tinyurl.com/h3jh4kt
[fn:bssl] https://en.wikipedia.org/wiki/Bessel's_correction
[fn:busi] https://en.wikipedia.org/wiki/Business_logic
[fn:cdot] We sometimes use the center dot or the $\times$ symbols to clarify
matrix multiplication. They have no other significance and we can always write
matrix multiplication just by juxtaposing the matrices.
[fn:clos] https://en.wikipedia.org/wiki/Closure_(computer_programming)
[fn:cold] This convention only models so-called /cold observables/, but it's enough to demonstrate Kalman's working over them.
[fn:cons] This is quite similar to the standard --- not  Wolfram's --- definition of a list as a pair of a value and of another list.
[fn:cova] We use the terms /covariance/ for matrices and /variance/ for scalars.
[fn:csoc] https://en.wikipedia.org/wiki/Separation_of_concerns
[fn:ctsc] https://en.wikipedia.org/wiki/Catastrophic_cancellation
[fn:dstr] http://tinyurl.com/ze6qfb3
[fn:elib] Brookner, Eli. Tracking and Kalman Filtering Made Easy, New York: Wiley, 1998. http://tinyurl.com/h8see8k
[fn:folk] http://ipnpr.jpl.nasa.gov/progress_report/42-178/178C.pdf
[fn:fldl] http://tinyurl.com/jmxsevr
[fn:fwik] https://en.wikipedia.org/wiki/Fold_%28higher-order_function%29
[fn:gama] https://en.wikipedia.org/wiki/Gauss%E2%80%93Markov_theorem
[fn:intr] http://introtorx.com/
[fn:jplg] JPL Geodynamics Program http://www.jpl.nasa.gov/report/1981.pdf
[fn:just] justified by the fact that $\mathbold{D}$ is a diagonal
matrix that commutes with all other products, therefore its left and right
inverses are equal and can be written as a reciprocal; in fact, $\mathbold{D}$
is a $1\times{1}$ matrix --- effectively a scalar --- in all examples in this paper
[fn:klde] B. Beckman, /Kalman Folding 3: Derivations/, to appear.
[fn:klf1] B. Beckman, /Kalman Folding, Part 1/, http://vixra.org/abs/1606.0328.
[fn:klf2] B. Beckman, /Kalman Folding 2: Tracking and System Dynamics/, http://vixra.org/abs/1606.0348.
[fn:klf3] B. Beckman, /Kalman Folding 3: Derivations/, to appear.
[fn:klf4] B. Beckman, /Kalman Folding 4: Streams and Observables/, to appear.
[fn:klf5] B. Beckman, /Kalman Folding 5: Non-Linear Models and the EKF/, to appear.
[fn:klf6] B. Beckman, /Kalman Folding 6: Dimensional Analysis/, to appear.
[fn:layi] https://en.wikipedia.org/wiki/Fundamental_theorem_of_software_engineering
[fn:lmbd] Many languages use the keyword /lambda/ for such expressions; Wolfram
uses the name /Function/.
[fn:lmlf] https://en.wikipedia.org/wiki/Lambda_lifting
[fn:lssq] https://en.wikipedia.org/wiki/Least_squares
[fn:ltis] http://tinyurl.com/hhhcgca
[fn:matt] https://www.cs.kent.ac.uk/people/staff/dat/miranda/whyfp90.pdf
[fn:mcmc] https://en.wikipedia.org/wiki/Particle_filter
[fn:musc] http://www1.cs.dartmouth.edu/~doug/music.ps.gz
[fn:ndim] https://en.wikipedia.org/wiki/Nondimensionalization
[fn:patt] http://tinyurl.com/j5jzy69
[fn:pseu] http://tinyurl.com/j8gvlug
[fn:rasp] http://www.wolfram.com/raspberry-pi/
[fn:rcrn] https://en.wikipedia.org/wiki/Recurrence_relation
[fn:rsfr] http://rosettacode.org/wiki/Loops/Foreach
[fn:rxbk] http://www.introtorx.com/content/v1.0.10621.0/07_Aggregation.html
[fn:scan] and of Haskell's scans and folds, and Rx's scans and folds, /etc./
[fn:scla] http://tinyurl.com/hhdot36
[fn:scnd] A state-space form containing a position and derivative is commonplace
in second-order dynamics like Newton's Second Law. We usually employ state-space
form to reduce \(n\)-th-order differential equations to first-order differential
equations by stacking the dependent variable on $n-1$ of its derivatives in the
state vector.
[fn:scnl] http://learnyouahaskell.com/higher-order-functions
[fn:stsp] https://en.wikipedia.org/wiki/State-space_representation
[fn:uncl] The initial uncial (lower-case) letter signifies that /we/ wrote this function; it wasn't supplied by Wolfram.
[fn:wfld] http://reference.wolfram.com/language/ref/FoldList.html?q=FoldList
[fn:wlf1] http://tinyurl.com/nfz9fyo
[fn:wlf2] http://rebcabin.github.io/blog/2013/02/04/welfords-better-formula/
[fn:wolf] http://reference.wolfram.com/language/
[fn:zarc] Zarchan and Musoff, /Fundamentals of Kalman Filtering, A Practical
Approach, Fourth Edition/, Ch. 4


